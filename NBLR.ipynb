{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# **Aditya Khandare** <br>\r\n",
    "## ML HW#1 : Naive Bayes and Logistic Regression for Text Classification <br>\r\n",
    "## netid : **ark200000** <br>\r\n",
    "## utd email : **ark200000@utdallas.edu** <br>\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "\r\n",
    "\r\n",
    "import pandas as pd\r\n",
    "from os import listdir\r\n",
    "import os as os\r\n",
    "import nltk\r\n",
    "from sklearn.feature_extraction.text import CountVectorizer\r\n",
    "from nltk.stem import WordNetLemmatizer\r\n",
    "from nltk.stem.snowball import SnowballStemmer\r\n",
    "import math\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "import numpy as np\r\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\r\n",
    "\r\n",
    "#Please use this command to install nltk data if not already installed \r\n",
    "# nltk.download() \r\n",
    "\r\n",
    "rootDir = \"dataset\"\r\n",
    "enron1 = []\r\n",
    "enron4 = []\r\n",
    "hw1 = []"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "All the datasets are moved in the folder named \"dataset\". <br>\r\n",
    "This folder must be present in the root directory; where this ipynb is saved and will be executed.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def get_AllFilePaths():\r\n",
    "    filepaths = []\r\n",
    "    for directories, subdirs, files in os.walk(rootDir):\r\n",
    "        for f in files:\r\n",
    "            if f.endswith(\".txt\"):\r\n",
    "                filepaths.append((os.path.join(directories, f)))\r\n",
    "    return filepaths"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def get_Data(dataset, test_data_type):\r\n",
    "    filepaths = get_AllFilePaths()\r\n",
    "    data = []\r\n",
    "    for f in filepaths:\r\n",
    "        file = open(f,'r',encoding = \"latin1\")\r\n",
    "        eg = file.read()\r\n",
    "        file.close()\r\n",
    "        path = os.path.split(os.path.split(f)[0])[-1]\r\n",
    "        if(dataset in f and test_data_type in f):\r\n",
    "            data.append([('1' if \"ham\" in path.lower() else '0'), eg])\r\n",
    "    return data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "#Extracting the entire Training Data\r\n",
    "\r\n",
    "enron1_df = pd.DataFrame(get_Data(\"enron1\",\"train\"), columns=[\"is_ham\", \"_examples\"])\r\n",
    "enron4_df = pd.DataFrame(get_Data(\"enron4\",\"train\"), columns=[\"is_ham\", \"_examples\"])\r\n",
    "hw1_df = pd.DataFrame(get_Data(\"hw1\",\"train\"), columns=[\"is_ham\", \"_examples\"])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "#Extracting the entire Testing Data\r\n",
    "\r\n",
    "test_enron1_df = pd.DataFrame(get_Data(\"enron1\",\"test\"), columns=[\"is_ham\", \"_examples\"])\r\n",
    "test_enron4_df = pd.DataFrame(get_Data(\"enron4\",\"test\"), columns=[\"is_ham\", \"_examples\"])\r\n",
    "test_hw1_df = pd.DataFrame(get_Data(\"hw1\",\"test\"), columns=[\"is_ham\", \"_examples\"])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "#Use this function to preprocess the data. The function accepts parameters \"stemming\" and \"lemmatization\" \r\n",
    "#which can be set true in order to process the data using snowball stemmer or lemmatizer. By default the function will not use stemming or lemmatization techniques on the data.\r\n",
    "\r\n",
    "def preprocess(example, stemming = False, lemmatization = False):\r\n",
    "    \r\n",
    "    processed_example = nltk.word_tokenize(example)\r\n",
    "    processed_example = [word.lower() for word in processed_example if word.isalpha()]\r\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))\r\n",
    "    processed_example = [word for word in processed_example if word not in stopwords]\r\n",
    "    if stemming is True:\r\n",
    "        snow_stemmer = SnowballStemmer(language='english')\r\n",
    "        processed_example = [snow_stemmer.stem(word) for word in processed_example]\r\n",
    "    if lemmatization is True:\r\n",
    "        lemmatizer = WordNetLemmatizer()\r\n",
    "        processed_example = [lemmatizer.lemmatize(word) for word in processed_example]\r\n",
    "    return processed_example"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "#Preprocess Training data\r\n",
    "\r\n",
    "enron1_df['_examples']  = [preprocess(example, lemmatization=True) for example in enron1_df._examples]\r\n",
    "enron4_df['_examples']  = [preprocess(example, lemmatization=True) for example in enron4_df._examples]\r\n",
    "hw1_df['_examples']  = [preprocess(example, lemmatization=True) for example in hw1_df._examples]\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "#Preprocess Test data\r\n",
    "\r\n",
    "test_enron1_df['_examples']  = [preprocess(example, lemmatization=True) for example in test_enron1_df._examples]\r\n",
    "test_enron4_df['_examples']  = [preprocess(example, lemmatization=True) for example in test_enron4_df._examples]\r\n",
    "test_hw1_df['_examples']  = [preprocess(example, lemmatization=True) for example in test_hw1_df._examples]\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "#Function to create Bag Of Words model\r\n",
    "\r\n",
    "def bow(dataframe):\r\n",
    "    cv = CountVectorizer()\r\n",
    "    temp = dataframe\r\n",
    "    transformed_data = cv.fit_transform(\" \".join(x) for x in dataframe._examples)\r\n",
    "    dataframe=pd.DataFrame(transformed_data.toarray(),columns=cv.get_feature_names())\r\n",
    "    dataframe.insert(0, 'is_ham', temp.is_ham)\r\n",
    "    dataframe.insert(1, '_examples', temp._examples)\r\n",
    "    return dataframe"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "#Function to create Bernoulli model\r\n",
    "\r\n",
    "\r\n",
    "def bernoulli(df):\r\n",
    "    cv = CountVectorizer(binary=True)\r\n",
    "    temp = df\r\n",
    "    transformed_data = cv.fit_transform(\" \".join(x) for x in df._examples)\r\n",
    "    df=pd.DataFrame(transformed_data.toarray(),columns=cv.get_feature_names())\r\n",
    "    df.insert(0, 'is_ham', temp.is_ham)\r\n",
    "    df.insert(1, '_examples', temp._examples)\r\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# Creating Bag of words and Bernoulli models of HW1 dataset\r\n",
    "\r\n",
    "hw1_bow = bow(hw1_df)\r\n",
    "hw1_br = bernoulli(hw1_df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# Creating Bag of words and Bernoulli models of Enron 1 dataset\r\n",
    "\r\n",
    "enron1_bow = bow(enron1_df)\r\n",
    "enron1_br = bernoulli(enron1_df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# Creating Bag of words and Bernoulli models of Enron 4 dataset\r\n",
    "\r\n",
    "enron4_bow = bow(enron4_df)\r\n",
    "enron4_br = bernoulli(enron4_df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# Use this function to calculate the probability of message being spam and ham in the dataset\r\n",
    "\r\n",
    "def get_spamHamProbabilities(dataframe):\r\n",
    "  \r\n",
    "    prob_spam = dataframe[\"is_ham\"].value_counts()[1] / dataframe.shape[0] \r\n",
    "    prob_ham = dataframe[\"is_ham\"].value_counts()[0] / dataframe.shape[0]\r\n",
    "    return prob_spam, prob_ham"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# Calculating the probabilities of spam and ham for the respective datasets\r\n",
    "\r\n",
    "enron1_prob_spam , enron1_prob_ham = get_spamHamProbabilities(enron1_bow)\r\n",
    "enron4_prob_spam , enron4_prob_ham = get_spamHamProbabilities(enron4_bow)\r\n",
    "hw1_prob_spam , hw1_prob_ham = get_spamHamProbabilities(hw1_bow)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# Use this function to calculate the probabilities of all the tokens in the dataset. The function accepts 2 values for the parameter: model.\r\n",
    "# Use model = \"bernoulli\" if you want to calculate the probabilities of tokens of Bernoulli model \r\n",
    "# Use model = \"bow\" if you want to calculate the probabilities of tokend of Bag Of Words model\r\n",
    "\r\n",
    "def prob_vocab(dataframe, model):\r\n",
    "    alfa = 1\r\n",
    "    \r\n",
    "    count_ham_words_inAllEmails = 0\r\n",
    "    count_spam_words_inAllEmails = 0\r\n",
    "    vocab_size = 0\r\n",
    "\r\n",
    "    temp = dataframe\r\n",
    "    temp = temp.drop([\"_examples\"], axis=1)\r\n",
    "\r\n",
    "    spam_df = temp[temp[\"is_ham\"] == \"0\"]\r\n",
    "    ham_df = temp[temp[\"is_ham\"] == \"1\"]\r\n",
    "\r\n",
    "    spam_df = spam_df.drop([\"is_ham\"], axis = 1)\r\n",
    "    ham_df = ham_df.drop([\"is_ham\"], axis = 1)\r\n",
    "\r\n",
    "    spam_df = pd.DataFrame( spam_df.to_numpy().sum(axis = 0).reshape(1,-1), columns = spam_df.columns)\r\n",
    "    ham_df = pd.DataFrame( ham_df.to_numpy().sum(axis = 0).reshape(1,-1), columns = ham_df.columns)\r\n",
    "\r\n",
    "    vocab_size = len(dataframe.columns) - 2\r\n",
    "\r\n",
    "    if model == \"bernoulli\":\r\n",
    "\r\n",
    "        spam_email_count = len(dataframe[dataframe[\"is_ham\"] == \"0\"])\r\n",
    "        ham_email_count = len(dataframe[dataframe[\"is_ham\"] == \"1\"])\r\n",
    "\r\n",
    "        for col_name, col_data in spam_df.iteritems():\r\n",
    "            spam_df[col_name] = (spam_df[col_name] + 1) / (spam_email_count + 10)\r\n",
    "\r\n",
    "        for col_name, col_data in ham_df.iteritems():\r\n",
    "            ham_df[col_name] = (ham_df[col_name] + 1) / (ham_email_count + 2)\r\n",
    "\r\n",
    "    elif model == \"bow\":\r\n",
    "\r\n",
    "        for eg in range(dataframe.shape[0]):\r\n",
    "            if dataframe[\"is_ham\"][eg] == \"1\":\r\n",
    "                count_ham_words_inAllEmails = count_ham_words_inAllEmails + len(dataframe[\"_examples\"][eg])\r\n",
    "            elif dataframe[\"is_ham\"][eg] == \"0\":\r\n",
    "                count_spam_words_inAllEmails = count_spam_words_inAllEmails + len(dataframe[\"_examples\"][eg])\r\n",
    "        \r\n",
    "        for col_name, col_data in spam_df.iteritems():\r\n",
    "            spam_df[col_name] = (spam_df[col_name] + alfa) / (count_spam_words_inAllEmails + alfa*vocab_size)\r\n",
    "\r\n",
    "        for col_name, col_data in ham_df.iteritems():\r\n",
    "            ham_df[col_name] = (ham_df[col_name] + alfa) / (count_ham_words_inAllEmails + alfa*vocab_size)\r\n",
    "\r\n",
    "    prob_matrix = pd.concat([spam_df,ham_df], ignore_index=True)\r\n",
    "    return prob_matrix\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# Calculating the probabilities of tokens for the Bag of Words Model\r\n",
    "\r\n",
    "enron1_bow_probability_vocab = prob_vocab(enron1_bow, \"bow\")\r\n",
    "enron4_bow_probability_vocab = prob_vocab(enron4_bow, \"bow\")\r\n",
    "hw1_bow_probability_vocab = prob_vocab(hw1_bow, \"bow\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "# Calculating the probabilities of tokens for the Bernoulli model\r\n",
    "\r\n",
    "\r\n",
    "enron1_br_probability_vocab = prob_vocab(enron1_br , \"bernoulli\")\r\n",
    "enron4_br_probability_vocab = prob_vocab(enron4_br , \"bernoulli\")\r\n",
    "hw1_br_probability_vocab = prob_vocab(hw1_br , \"bernoulli\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Algorithm used to train the multinomial naive bayes:\r\n",
    "\r\n",
    "\r\n",
    "## TRAINMULTINOMIALNB(C, D) <br>\r\n",
    "1. V ← EXTRACTVOCABULARY(D) <br>\r\n",
    "2. N ←COUNTDOCS(D)<br>\r\n",
    "3. for each c ∈C<br>\r\n",
    "4. do Nc ←COUNTDOCSINCLASS (D, c)<br>\r\n",
    "5. prior[c] ← Nc/N<br>\r\n",
    "6. textc ← CONCATENATETEXTOFALLDOCSINCLASS(D, c)<br>\r\n",
    "7. for each t ∈V<br>\r\n",
    "8. do Tct ← COUNTTOKENSOFTERM(textc, t)<br>\r\n",
    "9. for each t ∈V<br>\r\n",
    "10. do condprob[t][c] ←Tct+1<br>\r\n",
    "∑t′(Tct′+1)<br>\r\n",
    "11. return V, prior, condprob<br>\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "# Use this function for Multinomial Naive Bayes classification. All the calculations are done in log scale in order to avoid underflow.\r\n",
    "# \r\n",
    "\r\n",
    "def multinomial_naiveBayes(message, prob_spam, prob_ham, prob_vocab):\r\n",
    "    prob_spam = math.log(prob_spam)\r\n",
    "    prob_ham = math.log(prob_ham)\r\n",
    "    # prob_spam = (prob_spam)\r\n",
    "    # prob_ham = (prob_ham)\r\n",
    "    for word in message:\r\n",
    "        if word in prob_vocab.columns.to_list():\r\n",
    "            prob_spam += math.log(prob_vocab[word][0])\r\n",
    "            # prob_spam *= (prob_vocab[word][0])\r\n",
    "            prob_ham += math.log(prob_vocab[word][1])\r\n",
    "            # prob_ham *= (prob_vocab[word][1])\r\n",
    "        else:\r\n",
    "             prob_spam += math.log(1)\r\n",
    "             prob_ham += math.log(1)\r\n",
    "            # prob_spam *= (1)\r\n",
    "            # prob_ham *= (1)\r\n",
    "    if prob_ham > prob_spam:\r\n",
    "        return 'ham'\r\n",
    "    elif prob_ham < prob_spam:\r\n",
    "        return 'spam'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Algorithm used to train the Bernoulli Naive Bayes:\r\n",
    "\r\n",
    "\r\n",
    "## TRAINBERNOULLINB(C, D)\r\n",
    "1. V ←EXTRACTVOCABULARY(D)\r\n",
    "2. N ← COUNTDOCS(D)\r\n",
    "3. for each c ∈C\r\n",
    "4. do Nc ←COUNTDOCSINCLASS(D, c)\r\n",
    "5. prior[c] ← Nc/N\r\n",
    "6. for each t ∈V\r\n",
    "7. do Nct ←COUNTDOCSINCLASSCONTAININGTERM (D, c, t)\r\n",
    "8. condprob[t][c] ←(Nct + 1)/(Nc + 2)\r\n",
    "9. return V, prior, condprob"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "def bernoulli_naiveBayes(message, prob_spam, prob_ham, prob_vocab):\r\n",
    "    prob_spam = math.log(prob_spam)\r\n",
    "    prob_ham = math.log(prob_ham)\r\n",
    "\r\n",
    "    for col_name, col_data in prob_vocab.iteritems():\r\n",
    "        if col_name in message:\r\n",
    "            prob_spam += math.log(prob_vocab[col_name][0])\r\n",
    "            prob_ham += math.log(prob_vocab[col_name][1])\r\n",
    "        else:\r\n",
    "            prob_spam += math.log(1 - prob_vocab[col_name][0])\r\n",
    "            prob_ham += math.log(1 - prob_vocab[col_name][1])\r\n",
    "\r\n",
    "    if prob_ham > prob_spam:\r\n",
    "        return 'ham'\r\n",
    "    elif prob_ham < prob_spam:\r\n",
    "        return 'spam'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Algorithm used to apply the Multinomial Naive Bayes and Bernoulli Naive Bayes\r\n",
    "\r\n",
    "## APPLYMULTINOMIALNB(C, V, prior, condprob, d)<br>\r\n",
    "1. W ← EXTRACTTOKENSFROMDOC (V, d)<br>\r\n",
    "2. for each c ∈C<br>\r\n",
    "3. do score[c] ←log prior[c]<br>\r\n",
    "4. for each t ∈W<br>\r\n",
    "5. do score[c] += log condprob[t][c]<br>\r\n",
    "6. return arg maxc∈C score[c]<br>\r\n",
    "\r\n",
    "## APPLYBERNOULLINB(C, V, prior, condprob, d)\r\n",
    "1. Vd ← EXTRACTTERMSFROMDOC(V, d)\r\n",
    "2. for each c ∈C\r\n",
    "3. do score[c] ←log prior[c]\r\n",
    "4. for each t ∈V\r\n",
    "5. do if t ∈Vd\r\n",
    "6. then score[c] += log condprob[t][c]\r\n",
    "7. else score[c] += log(1 −condprob[t][c])\r\n",
    "8. return arg maxc∈C score[c]\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "def run_model(dataframe, prob_spam, prob_ham, prob_vocab ,model):\r\n",
    "    accuracy = 0\r\n",
    "    tp = 0\r\n",
    "    tn = 0\r\n",
    "    fp = 0\r\n",
    "    fn = 0\r\n",
    "    for x in range(dataframe.shape[0]):\r\n",
    "        if model == \"bow\":\r\n",
    "            typ = multinomial_naiveBayes(dataframe[\"_examples\"][x], prob_spam, prob_ham,prob_vocab)\r\n",
    "        elif model == \"ber\":\r\n",
    "            typ = bernoulli_naiveBayes(dataframe[\"_examples\"][x], prob_spam, prob_ham,prob_vocab)\r\n",
    "\r\n",
    "        if typ == \"ham\" and dataframe[\"is_ham\"][x] == \"1\":\r\n",
    "            accuracy  += 1\r\n",
    "            tp += 1\r\n",
    "\r\n",
    "        elif typ == \"spam\" and dataframe[\"is_ham\"][x] == \"0\":\r\n",
    "            accuracy  += 1\r\n",
    "            tn += 1\r\n",
    "        elif typ == \"ham\" and dataframe[\"is_ham\"][x] == \"0\":\r\n",
    "            fp += 1\r\n",
    "        elif typ == \"spam\" and dataframe[\"is_ham\"][x] == \"1\":\r\n",
    "            fn += 1\r\n",
    "    print() \r\n",
    "    print(\"model : \", model)        \r\n",
    "    print(\"Accuracy : \", (accuracy/dataframe.shape[0])*100)\r\n",
    "    print(\"Precision : \", (tp / (tp+fp))*100)\r\n",
    "    print(\"Recall : \", (tp / (tp+fn))*100)\r\n",
    "    print(\"F1 score : \", 2*(((tp / (tp+fp))*100)*(tp / (tp+fn))*100)/(((tp / (tp+fp))*100)+(tp / (tp+fn))*100))\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "# Finally running the Discrete and Multinomial Naive Bayes on Enron 1, Enron 4 and HW 1 datasets\r\n",
    "\r\n",
    "print(\"ENRON 1 Dataset: \") \r\n",
    "run_model(test_enron1_df,enron1_prob_spam, enron1_prob_ham, enron1_bow_probability_vocab, model=\"bow\")\r\n",
    "run_model(test_enron1_df,enron1_prob_spam, enron1_prob_ham, enron1_bow_probability_vocab, model=\"ber\")\r\n",
    "\r\n",
    "print(\"ENRON 4 Dataset: \") \r\n",
    "run_model(test_enron4_df,enron4_prob_spam, enron4_prob_ham, enron4_bow_probability_vocab, model=\"bow\")\r\n",
    "run_model(test_enron4_df,enron4_prob_spam, enron4_prob_ham, enron4_bow_probability_vocab, model=\"ber\")\r\n",
    "\r\n",
    "print(\"HW 1 Dataset: \") \r\n",
    "run_model(test_hw1_df,hw1_prob_spam, hw1_prob_ham, hw1_bow_probability_vocab, model=\"bow\")\r\n",
    "run_model(test_hw1_df,hw1_prob_spam, hw1_prob_ham, hw1_bow_probability_vocab, model=\"ber\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ENRON 1 Dataset: \n",
      "\n",
      "model :  bow\n",
      "Accuracy :  92.98245614035088\n",
      "Precision :  93.65079365079364\n",
      "Recall :  96.09120521172639\n",
      "F1 score :  94.85530546623792\n",
      "\n",
      "model :  ber\n",
      "Accuracy :  93.2017543859649\n",
      "Precision :  93.67088607594937\n",
      "Recall :  96.41693811074919\n",
      "F1 score :  95.02407704654897\n",
      "ENRON 4 Dataset: \n",
      "\n",
      "model :  bow\n",
      "Accuracy :  94.47513812154696\n",
      "Precision :  86.30952380952381\n",
      "Recall :  95.39473684210526\n",
      "F1 score :  90.62500000000001\n",
      "\n",
      "model :  ber\n",
      "Accuracy :  93.73848987108656\n",
      "Precision :  85.97560975609755\n",
      "Recall :  92.76315789473685\n",
      "F1 score :  89.24050632911393\n",
      "HW 1 Dataset: \n",
      "\n",
      "model :  bow\n",
      "Accuracy :  94.56066945606695\n",
      "Precision :  95.48022598870057\n",
      "Recall :  97.12643678160919\n",
      "F1 score :  96.29629629629629\n",
      "\n",
      "model :  ber\n",
      "Accuracy :  94.56066945606695\n",
      "Precision :  95.48022598870057\n",
      "Recall :  97.12643678160919\n",
      "F1 score :  96.29629629629629\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# MCAP Logistic Regression:\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "def split_dataset(dataset, size):\r\n",
    "    train,test = train_test_split(dataset, test_size=size/100)\r\n",
    "    return train.reset_index(drop = True), test.reset_index(drop = True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "# Splitting the datasets into 70:30 ratio for training and validation purposes. Please note that the validation data has been referred as test in the below variables. \r\n",
    "# However the variables contain the validation data and not test data.\r\n",
    "\r\n",
    "enron1_lr_train_df, enron1_lr_test_df = split_dataset(enron1_df , 30)\r\n",
    "enron4_lr_train_df, enron4_lr_test_df = split_dataset(enron4_df , 30)\r\n",
    "hw1_lr_train_df, hw1_lr_test_df = split_dataset(hw1_df ,30)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "# Creating Bag of Words and Bernoulli models of the training data\r\n",
    "# We will also be creating the X and Y vectors required for LR\r\n",
    "\r\n",
    "enron1_lr_train_bow = bow(enron1_lr_train_df)\r\n",
    "enron1_lr_train_bow_X = enron1_lr_train_bow.iloc[:,2:].to_numpy()\r\n",
    "enron1_lr_train_bow_Y = enron1_lr_train_bow[\"is_ham\"].to_numpy().astype(int)\r\n",
    "\r\n",
    "enron1_lr_train_br = bernoulli(enron1_lr_train_df)\r\n",
    "enron1_lr_train_br_X = enron1_lr_train_bow.iloc[:,2:].to_numpy()\r\n",
    "enron1_lr_train_br_Y = enron1_lr_train_bow[\"is_ham\"].to_numpy().astype(int)\r\n",
    "\r\n",
    "enron4_lr_train_bow = bow(enron4_lr_train_df)\r\n",
    "enron4_lr_train_bow_X = enron4_lr_train_bow.iloc[:,2:].to_numpy()\r\n",
    "enron4_lr_train_bow_Y = enron4_lr_train_bow[\"is_ham\"].to_numpy().astype(int)\r\n",
    "\r\n",
    "enron4_lr_train_br = bernoulli(enron4_lr_train_df)\r\n",
    "enron4_lr_train_br_X = enron4_lr_train_br.iloc[:,2:].to_numpy()\r\n",
    "enron4_lr_train_br_Y = enron4_lr_train_br[\"is_ham\"].to_numpy().astype(int)\r\n",
    "\r\n",
    "hw1_lr_train_bow = bow(hw1_lr_train_df)\r\n",
    "hw1_lr_train_bow_X = hw1_lr_train_bow.iloc[:,2:].to_numpy()\r\n",
    "hw1_lr_train_bow_Y = hw1_lr_train_bow[\"is_ham\"].to_numpy().astype(int)\r\n",
    "\r\n",
    "\r\n",
    "hw1_lr_train_br = bernoulli(hw1_lr_train_df)\r\n",
    "hw1_lr_train_br_X = hw1_lr_train_br.iloc[:,2:].to_numpy()\r\n",
    "hw1_lr_train_br_Y = hw1_lr_train_br[\"is_ham\"].to_numpy().astype(int)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "# The commomnly used functions are defined below:\r\n",
    "\r\n",
    "def get_exponential(vector1, vector2):\r\n",
    "    return np.exp(np.dot(vector2,vector1))\r\n",
    "\r\n",
    "def get_dotProduct(vector1, vector2):\r\n",
    "    return np.dot(vector2,vector1)\r\n",
    "\r\n",
    "def prob_Y0(W, X):\r\n",
    "    return 1/(1+get_exponential(W, X))\r\n",
    "\r\n",
    "def prob_Y1(W, X):\r\n",
    "    return 1-prob_Y0(W, X)\r\n",
    "\r\n",
    "def get_conditionalLogLikelihood(W, X, Y, lmbda=0): \r\n",
    "    return np.sum((Y*get_dotProduct(W, X))+np.log(prob_Y1(W, X)))-np.dot(W.T,W)*lmbda/2\r\n",
    "    # return np.sum(Y*np.log(prob_Y0(W,X)) + (1 - Y)*(np.log(prob_Y1(W,X))))\r\n",
    "\r\n",
    "def compare_dotProductOfvectors(vector1, vector2):\r\n",
    "    # if get_dotProduct(vector1, vector2) > 0:\r\n",
    "    #     return 1\r\n",
    "    # else:\r\n",
    "    #     return 0\r\n",
    "    return (get_dotProduct(vector1, vector2)>0).astype(int)\r\n",
    " \r\n",
    "    \r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "# Function to calculate the Weights using Gradient Ascent \r\n",
    "\r\n",
    "def get_gradientAscent(weights, VectorX, VectorY, lmbda=0.1, learningRate=0.01, iterations=500):\r\n",
    "    for i in range(iterations):\r\n",
    "        weights += learningRate*(np.dot(VectorX.T,(VectorY-prob_Y1(weights, VectorX)))-(lmbda*weights))\r\n",
    "    return weights"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "def mcap_lr(testData ,vocab, W, model):\r\n",
    "    tp = 0\r\n",
    "    tn = 0\r\n",
    "    fp = 0\r\n",
    "    fn = 0\r\n",
    "    test_count = 0\r\n",
    "    acc = 0\r\n",
    "    for i in range(testData.shape[0]):\r\n",
    "        features = np.zeros((1,vocab.shape[1] - 1))\r\n",
    "        features[0][0] = 1\r\n",
    "        test_count = 0\r\n",
    "        acc = 0\r\n",
    "        for word in testData[\"_examples\"][i]:\r\n",
    "            acc += 1\r\n",
    "            if word in vocab.columns:\r\n",
    "                test_count += 1 \r\n",
    "                if model == \"bow\":\r\n",
    "                    features[0][vocab.columns.get_loc(word) - 1] += 1\r\n",
    "                elif model == \"ber\":\r\n",
    "                    features[0][vocab.columns.get_loc(word) - 1] = 1\r\n",
    "        cals = compare_dotProductOfvectors(W,features)\r\n",
    "        # print(test_count / acc)\r\n",
    "        # print(int(testData[\"is_ham\"][i]))\r\n",
    "        # print((cals[0][0]))\r\n",
    "        predicted = cals[0][0]\r\n",
    "        actual = int(testData[\"is_ham\"][i])\r\n",
    "        if predicted == 1 and actual == 1:\r\n",
    "            tp += 1\r\n",
    "        elif predicted  == 1 and actual == 0:\r\n",
    "            fp += 1\r\n",
    "        elif predicted  == 0 and actual == 0:\r\n",
    "            tn += 1\r\n",
    "        elif predicted  == 0 and actual == 1:\r\n",
    "            fn += 1\r\n",
    "\r\n",
    "\r\n",
    "    \r\n",
    "    final_accuracy = ((tp + tn) / testData.shape[0])*100\r\n",
    "    print() \r\n",
    "    print(\"model : \", model)        \r\n",
    "    print(\"Accuracy : \", ((tp+tn)/testData.shape[0])*100)\r\n",
    "    print(\"Precision : \", (tp / (tp+fp))*100)\r\n",
    "    print(\"Recall : \", (tp / (tp+fn))*100)\r\n",
    "    print(\"F1 score : \", 2*(((tp / (tp+fp))*100)*(tp / (tp+fn))*100)/(((tp / (tp+fp))*100)+(tp / (tp+fn))*100))\r\n",
    "\r\n",
    "    print()\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "def run_lr(testingData , X, Y, vocab, lmbda = 0.005 , lr = 0.01 , iterations = 1000 , model = \"bow\" ):\r\n",
    "   \r\n",
    "    X0=np.ones((X.shape[0],1), dtype=int)\r\n",
    "    X = np.hstack((X0, X))\r\n",
    "   \r\n",
    "    Y.shape=(X.shape[0],1)\r\n",
    "\r\n",
    "    # W=np.random.rand(X.shape[1], 1)\r\n",
    "    W = np.zeros((X.shape[1], 1))\r\n",
    "    W = get_gradientAscent(W, X, Y, lmbda ,lr , iterations)\r\n",
    "\r\n",
    "    mcap_lr(testingData , vocab, W, model)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "print(\"ENRON 1\")\r\n",
    "run_lr(enron1_lr_test_df , enron1_lr_train_bow_X , enron1_lr_train_bow_Y , enron1_lr_train_bow)\r\n",
    "run_lr(enron1_lr_test_df , enron1_lr_train_br_X , enron1_lr_train_br_Y , enron1_lr_train_br , model = \"ber\")\r\n",
    "\r\n",
    "\r\n",
    "print(\"ENRON 4\")\r\n",
    "run_lr(enron4_lr_test_df , enron4_lr_train_bow_X , enron4_lr_train_bow_Y , enron4_lr_train_bow)\r\n",
    "run_lr(enron4_lr_test_df , enron4_lr_train_br_X , enron4_lr_train_br_Y ,enron4_lr_train_br , model = \"ber\")\r\n",
    "\r\n",
    "print(\"HW 1\")\r\n",
    "run_lr(hw1_lr_test_df , hw1_lr_train_bow_X , hw1_lr_train_bow_Y , hw1_lr_train_bow)\r\n",
    "run_lr(hw1_lr_test_df , hw1_lr_train_br_X , hw1_lr_train_br_Y , hw1_lr_train_br, model = \"ber\")\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ENRON 1\n",
      "\n",
      "model :  bow\n",
      "Accuracy :  97.03703703703704\n",
      "Precision :  98.9795918367347\n",
      "Recall :  97.0\n",
      "F1 score :  97.97979797979798\n",
      "\n",
      "\n",
      "model :  ber\n",
      "Accuracy :  94.81481481481482\n",
      "Precision :  97.9381443298969\n",
      "Recall :  95.0\n",
      "F1 score :  96.4467005076142\n",
      "\n",
      "ENRON 4\n",
      "\n",
      "model :  bow\n",
      "Accuracy :  95.03105590062113\n",
      "Precision :  96.55172413793103\n",
      "Recall :  80.0\n",
      "F1 score :  87.5\n",
      "\n",
      "\n",
      "model :  ber\n",
      "Accuracy :  95.65217391304348\n",
      "Precision :  96.66666666666667\n",
      "Recall :  82.85714285714286\n",
      "F1 score :  89.23076923076924\n",
      "\n",
      "HW 1\n",
      "\n",
      "model :  bow\n",
      "Accuracy :  91.36690647482014\n",
      "Precision :  94.89795918367348\n",
      "Recall :  93.0\n",
      "F1 score :  93.93939393939395\n",
      "\n",
      "\n",
      "model :  ber\n",
      "Accuracy :  89.20863309352518\n",
      "Precision :  91.2621359223301\n",
      "Recall :  94.0\n",
      "F1 score :  92.61083743842363\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "print(\"ENRON 1\")\r\n",
    "run_lr(test_enron1_df , enron1_lr_train_bow_X , enron1_lr_train_bow_Y , enron1_lr_train_bow)\r\n",
    "run_lr(test_enron1_df , enron1_lr_train_br_X , enron1_lr_train_br_Y , enron1_lr_train_br , model = \"ber\")\r\n",
    "\r\n",
    "\r\n",
    "print(\"ENRON 4\")\r\n",
    "run_lr(test_enron4_df , enron4_lr_train_bow_X , enron4_lr_train_bow_Y , enron4_lr_train_bow)\r\n",
    "run_lr(test_enron4_df , enron4_lr_train_br_X , enron4_lr_train_br_Y ,enron4_lr_train_br , model = \"ber\")\r\n",
    "\r\n",
    "print(\"HW 1\")\r\n",
    "run_lr(test_hw1_df , hw1_lr_train_bow_X , hw1_lr_train_bow_Y , hw1_lr_train_bow)\r\n",
    "run_lr(test_hw1_df , hw1_lr_train_br_X , hw1_lr_train_br_Y , hw1_lr_train_br, model = \"ber\")\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ENRON 1\n",
      "\n",
      "model :  bow\n",
      "Accuracy :  95.39473684210526\n",
      "Precision :  97.98657718120806\n",
      "Recall :  95.11400651465797\n",
      "F1 score :  96.52892561983471\n",
      "\n",
      "\n",
      "model :  ber\n",
      "Accuracy :  95.6140350877193\n",
      "Precision :  98.64406779661017\n",
      "Recall :  94.78827361563518\n",
      "F1 score :  96.67774086378738\n",
      "\n",
      "ENRON 4\n",
      "\n",
      "model :  bow\n",
      "Accuracy :  95.39594843462247\n",
      "Precision :  100.0\n",
      "Recall :  83.55263157894737\n",
      "F1 score :  91.0394265232975\n",
      "\n",
      "\n",
      "model :  ber\n",
      "Accuracy :  95.76427255985267\n",
      "Precision :  100.0\n",
      "Recall :  84.86842105263158\n",
      "F1 score :  91.8149466192171\n",
      "\n",
      "HW 1\n",
      "\n",
      "model :  bow\n",
      "Accuracy :  93.72384937238493\n",
      "Precision :  96.49122807017544\n",
      "Recall :  94.82758620689656\n",
      "F1 score :  95.65217391304347\n",
      "\n",
      "\n",
      "model :  ber\n",
      "Accuracy :  93.93305439330544\n",
      "Precision :  95.44159544159544\n",
      "Recall :  96.26436781609196\n",
      "F1 score :  95.85121602288983\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('hw1': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "interpreter": {
   "hash": "531b0134987c0aafae321ed1cd8f9e42c457322123fad9ee871b5aa55e2a1c2c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}